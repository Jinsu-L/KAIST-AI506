{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codes for auto-reload\n",
    "# You don't need to re-import after modifying the modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Make the relations based on Jac-Similarity\n",
    "\n",
    "It takes too much time (about 1h) So, we recommend using the result.  \n",
    "We enclose item_to_item_its_jac.csv and item_to_item_its_jac_with_user_item.csv in ./Dataset folder and recommend using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Set\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = \"./Dataset\"\n",
    "user_item = pd.read_csv('./Dataset/user_item.csv', delimiter=',', names=['user_id', 'item_id'])\n",
    "total_number_user: int = 53897\n",
    "total_number_itemset: int = 27694\n",
    "total_number_item: int = 42653\n",
    "\n",
    "user_total_id: Set = set([i for i in range(total_number_user)])\n",
    "user_item_user_id_set: Set = set(user_item['user_id'])\n",
    "user_item_item_id_set: Set = set(user_item['item_id'])\n",
    "nonmatching_user_count: Set = user_total_id - user_item_user_id_set\n",
    "\n",
    "itemset_item_training = pd.read_csv('./Dataset/itemset_item_training.csv', delimiter=',', names=['itemset_id', 'item_id'])\n",
    "itemset_item_id_set: Set = set(itemset_item_training['item_id'])\n",
    "user_item_id_set: Set = set(user_item['item_id'])\n",
    "total_item_id_set: Set = user_item_id_set.union(itemset_item_id_set)\n",
    "\n",
    "row_connect_itemset_to_item_dataframe = itemset_item_training.groupby('itemset_id', as_index=False)['item_id'].agg(lambda x: list(sorted(x)))\n",
    "row_connect_itemset_to_item_dataframe['item_count'] = row_connect_itemset_to_item_dataframe['item_id'].apply(lambda x: len(x))\n",
    "\n",
    "row_connect_item_to_itemset_dataframe = itemset_item_training.groupby('item_id', as_index=False)['itemset_id'].agg(lambda x: list(sorted(x)))\n",
    "row_connect_item_to_itemset_dataframe['itemset_count'] = row_connect_item_to_itemset_dataframe['itemset_id'].apply(lambda x: len(x))\n",
    "\n",
    "row_connect_itemset_to_item_dataframe.insert(1, 'iset_id', row_connect_itemset_to_item_dataframe['itemset_id'])\n",
    "row_connect_itemset_to_item_dataframe.set_index('iset_id', inplace=True)\n",
    "\n",
    "for i in list(set(range(total_number_itemset)) - set(row_connect_itemset_to_item_dataframe['itemset_id'])):\n",
    "    row_connect_itemset_to_item_dataframe.loc[i] = [i, [], 0]\n",
    "row_connect_itemset_to_item_dataframe = row_connect_itemset_to_item_dataframe.sort_index()\n",
    "\n",
    "ii_itemset_sim_check = list(row_connect_itemset_to_item_dataframe.itertuples(index=False))\n",
    "ii_item_sim_check = list(row_connect_item_to_itemset_dataframe.itertuples(index=False))\n",
    "\n",
    "ii_itemset_sim_check.sort(key=lambda x: x[0])\n",
    "ii_item_sim_check.sort(key=lambda x: x[0])\n",
    "\n",
    "jaccard_similarity = lambda s1, s2: len(s1 & s2) / len(s1 | s2)\n",
    "overlap_similarity = lambda s1, s2: len(s1 & s2) / min(len(s1), len(s2))\n",
    "\n",
    "connection_dict_jaccard_sim = defaultdict(float)\n",
    "\n",
    "print(\"Make the relations based on itemset-item relation\")\n",
    "for itemset_data in tqdm(ii_itemset_sim_check):\n",
    "    for i in range(len(itemset_data[1])):\n",
    "        for j in range(i+1, len(itemset_data[1])):\n",
    "            connection_dict_jaccard_sim[(ii_item_sim_check[itemset_data[1][i]][0], \n",
    "                                         ii_item_sim_check[itemset_data[1][j]][0])] = jaccard_similarity(set(ii_item_sim_check[itemset_data[1][i]][1]), \n",
    "                                                                                                      set(ii_item_sim_check[itemset_data[1][j]][1]))\n",
    "\n",
    "print(\"Save the relations based on itemset-item relation\")\n",
    "with open(path + \"/item_to_item_its_jac.csv\", 'w') as f:\n",
    "    for key, value in connection_dict_jaccard_sim.items():\n",
    "        f.write(f'{int(key[0])},{int(key[1])},{value}\\n')\n",
    "\n",
    "jaccard_similarity = lambda s1, s2: len(s1 & s2) / len(s1 | s2)\n",
    "\n",
    "# relation info\n",
    "train_file = path + '/user_item.csv'\n",
    "user_item = pd.read_csv(train_file, delimiter=',', names=['user_id', 'item_id'])\n",
    "\n",
    "row_connect_user_to_item_dataframe = user_item.groupby('user_id', as_index=False)['item_id'].agg(lambda x: list(sorted(x)))\n",
    "row_connect_user_to_item_dataframe['item_count'] = row_connect_user_to_item_dataframe['item_id'].apply(lambda x: len(x))\n",
    "\n",
    "row_connect_item_to_user_dataframe = user_item.groupby('item_id', as_index=False)['user_id'].agg(lambda x: list(sorted(x)))\n",
    "row_connect_item_to_user_dataframe['user_count'] = row_connect_item_to_user_dataframe['user_id'].apply(lambda x: len(x))\n",
    "\n",
    "row_connect_user_to_item_dataframe.insert(1, 'u_id', row_connect_user_to_item_dataframe['user_id'])\n",
    "row_connect_user_to_item_dataframe.set_index('u_id', inplace=True)\n",
    "\n",
    "row_connect_item_to_user_dataframe.insert(1, 'i_id', row_connect_item_to_user_dataframe['item_id'])\n",
    "row_connect_item_to_user_dataframe.set_index('i_id', inplace=True)\n",
    "\n",
    "for i in list(set(range(total_number_user)) - set(row_connect_user_to_item_dataframe['user_id'])):\n",
    "    row_connect_user_to_item_dataframe.loc[i] = [i, [], 0]\n",
    "row_connect_user_to_item_dataframe = row_connect_user_to_item_dataframe.sort_index()\n",
    "\n",
    "for i in list(set(range(total_number_item)) - set(row_connect_item_to_user_dataframe['item_id'])):\n",
    "    row_connect_item_to_user_dataframe.loc[i] = [i, [], 0]\n",
    "row_connect_item_to_user_dataframe = row_connect_item_to_user_dataframe.sort_index()\n",
    "\n",
    "ui_user_sim_check = list(row_connect_user_to_item_dataframe.itertuples(index=False))\n",
    "ui_item_sim_check = list(row_connect_item_to_user_dataframe.itertuples(index=False))\n",
    "\n",
    "ui_user_sim_check.sort(key=lambda x: x[0])\n",
    "ui_item_sim_check.sort(key=lambda x: x[0])\n",
    "connection_dict_jaccard_sim = defaultdict(float)\n",
    "\n",
    "item_value = defaultdict(float)\n",
    "for item_data in tqdm(ui_user_sim_check):\n",
    "    for i in range(len(item_data[1])):\n",
    "        for j in range(i+1, len(item_data[1])):\n",
    "            item_value[(ui_item_sim_check[item_data[1][i]][0], ui_item_sim_check[item_data[1][j]][0])] = jaccard_similarity(set(ui_item_sim_check[item_data[1][i]][1]), set(ui_item_sim_check[item_data[1][j]][1]))\n",
    "\n",
    "with open(path + \"/item_to_item_its_jac_with_user_item.csv\", \"w\") as w:\n",
    "    for (i, j), score in item_value.items():\n",
    "        w.write(\"%d,%d,%f\\n\" %(i, j, score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Maximum Spanning Tree (MST)  \n",
    "  \n",
    "We enclose mst.csv in ./Dataset folder and recommend using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.mst import MST\n",
    "\n",
    "mst = MST('./Dataset/item_to_item_its_jac_with_user_item.csv')\n",
    "mst_make = mst.kruskal()\n",
    "with open('./Dataset/mst.csv', 'w') as f:\n",
    "    for i, j, _ in mst_make:\n",
    "        f.write(f'{i},{j}\\n')\n",
    "        \n",
    "extended_style = mst.extended_style_graph(mst_make)\n",
    "with open('./Dataset/item_by_item_its_jac_with_mst.csv', 'w') as f:\n",
    "    for (i, j), w in extended_style.items():\n",
    "        f.write(f'{i},{j},{w}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hierarchical Community Detection  \n",
    "  \n",
    "The number of communities can vary depending on louvain, so the code was written based on the total number of clusters being 11776.  \n",
    "We enclose louvain_community_feature.csv in ./Dataset folder and recommend using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.item_community import community\n",
    "\n",
    "com = community(weight=True)\n",
    "louvain_community_list_each_step, hierarchical_community_data_each_item = com.make_hierarchical_community()\n",
    "\n",
    "with open('./Dataset/louvain_community_feature.csv', 'w') as f:\n",
    "    for i in hierarchical_community_data_each_item:\n",
    "        f.write(f\"{','.join([str(i)]+[str(item) for item in hierarchical_community_data_each_item[i]])}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get Item Embedding of CASG GNN  \n",
    "  \n",
    "We enclose CASG_GNN_ITEM_OUT.npy in ./Dataset folder and recommend using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "# from pytorch_lightning.metrics.functional import accuracy\n",
    "from tqdm import tqdm\n",
    "from Datasets import Task2Dataset\n",
    "from Models.Task2 import Task2Net\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "###########################################################\n",
    "batch_size = 65536\n",
    "lr = 1e-3\n",
    "epoch = 3000\n",
    "scheduler_step = 300\n",
    "scheduler_decay = 0.9\n",
    "\n",
    "resample_rate = 0.35 # dataset에서 label로 쓰일 비율\n",
    "resample_epoch = 10\n",
    "\n",
    "save_folder = \"./GCN_3L_CASG\"\n",
    "\n",
    "###########################################################\n",
    "\n",
    "dataset = Task2Dataset(\"Dataset/\")\n",
    "dataset.resample(resample_rate)\n",
    "g = dataset.g.to(device)\n",
    "train_g = dataset.train_g.to(device)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model = Task2Net(dataset.g.to(device)).to(device)\n",
    "\n",
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_decay)\n",
    "\n",
    "# Our model\n",
    "checkpoint = torch.load('CASG_model_0.1437.tar')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "print(model)\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "for itemset_id, query, pos, neg, query_items, lengths in train_dataloader:\n",
    "    pass\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    labels = []\n",
    "    batched_query = []\n",
    "    query_items = []\n",
    "    lengths = []\n",
    "    for itemset_id, querys in dataset.valid_itemset_items.items():\n",
    "        query = np.zeros(dataset.n_items, dtype=np.float32)\n",
    "        label = dataset.valid_itemset_label[itemset_id]\n",
    "        query[querys] = 1\n",
    "\n",
    "        batched_query.append(query)\n",
    "        labels.append(label)\n",
    "        query_items.append(querys + [0 for _ in range(5 - len(querys))])\n",
    "        lengths.append(len(querys))\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    query_embeds, pos_embeds, _, logit = model(g, torch.tensor(batched_query).to(device), \n",
    "                                               torch.tensor(list(range(dataset.n_items))).to(device), \n",
    "                                               neg.to(device), \n",
    "                                               torch.tensor(query_items).to(device), \n",
    "                                               torch.tensor(lengths).to(device))\n",
    "    np.save(\"./Dataset/CASG_GNN_ITEM_OUT.npy\", pos_embeds.detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cluster_cuda_11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
