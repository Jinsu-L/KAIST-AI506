{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bi-LSTM, DAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Input, Embedding, Flatten, Dropout, concatenate, Activation, RepeatVector, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from tensorflow import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import gc\n",
    "\n",
    "itemset_item_training = pd.read_csv('./Dataset/itemset_item_training.csv', delimiter=',', names=['itemset_id', 'item_id'])\n",
    "\n",
    "row_connect_itemset_to_item_dataframe = itemset_item_training.groupby('itemset_id', as_index=False)['item_id'].agg(lambda x: list(sorted(x)))\n",
    "row_connect_itemset_to_item_dataframe['item_count'] = row_connect_itemset_to_item_dataframe['item_id'].apply(lambda x: len(x))\n",
    "\n",
    "row_connect_item_to_itemset_dataframe = itemset_item_training.groupby('item_id', as_index=False)['itemset_id'].agg(lambda x: list(sorted(x)))\n",
    "row_connect_item_to_itemset_dataframe['itemset_count'] = row_connect_item_to_itemset_dataframe['itemset_id'].apply(lambda x: len(x))\n",
    "\n",
    "row_connect_itemset_to_item_dataframe.insert(1, 'iset_id', row_connect_itemset_to_item_dataframe['itemset_id'])\n",
    "row_connect_itemset_to_item_dataframe.set_index('iset_id', inplace=True)\n",
    "\n",
    "# train에서 1번도 출현하지 않은 itemset index에 대해 row 삽입\n",
    "for i in list(set(range(27694)) - set(row_connect_itemset_to_item_dataframe['itemset_id'])):\n",
    "    row_connect_itemset_to_item_dataframe.loc[i] = [i, [], 0]\n",
    "row_connect_itemset_to_item_dataframe = row_connect_itemset_to_item_dataframe.sort_index()\n",
    "\n",
    "ii_itemset_sim_check = list(row_connect_itemset_to_item_dataframe.itertuples(index=False))\n",
    "ii_item_sim_check = list(row_connect_item_to_itemset_dataframe.itertuples(index=False))\n",
    "\n",
    "ii_itemset_sim_check.sort(key=lambda x: x[0])\n",
    "ii_item_sim_check.sort(key=lambda x: x[0])\n",
    "\n",
    "check_appearance = []\n",
    "with open('./Dataset/itemset_item_valid_query.csv', 'r') as f:\n",
    "    for row in f.readlines(): \n",
    "        itemset, item = row.strip().split(',')\n",
    "        check_appearance.append(int(item))\n",
    "\n",
    "check_appearance.sort(reverse=True)\n",
    "re_index = defaultdict(lambda: -1)\n",
    "index = 0\n",
    "while check_appearance:\n",
    "    re_index[check_appearance.pop()] = index\n",
    "    index += 1\n",
    "\n",
    "for i in range(42563):\n",
    "    if re_index[i] == -1:\n",
    "        re_index[i] = index\n",
    "        index += 1\n",
    "\n",
    "itemset_sequence = defaultdict(list)\n",
    "for item in ii_itemset_sim_check:\n",
    "    if len(item[1]):\n",
    "        random.shuffle(item[1])\n",
    "        itemset_sequence[item[0]] = item[1]\n",
    "train_data = [value for value in itemset_sequence.values()]\n",
    "\n",
    "validset_dict = defaultdict(list)\n",
    "\n",
    "with open('./Dataset/itemset_item_valid_query.csv') as f:\n",
    "    for row in f.readlines():\n",
    "        key, value = map(float, row.strip().split(','))\n",
    "        validset_dict[int(key)].append(int(value))\n",
    "\n",
    "validset_list = [(key, value) for key, value in validset_dict.items()]\n",
    "validset_list.sort(key=lambda x: x[0])\n",
    "valid_data = [value[1] for value in validset_list]\n",
    "\n",
    "validset_answer_dict = defaultdict(list)\n",
    "with open('./Dataset/itemset_item_valid_answer.csv') as f:\n",
    "    for row in f.readlines():\n",
    "        key, value = map(float, row.strip().split(','))\n",
    "        validset_answer_dict[int(key)].append(int(value))\n",
    "\n",
    "validset_answer_list = [(key, value) for key, value in validset_answer_dict.items()]\n",
    "validset_answer_list.sort(key=lambda x: x[0])\n",
    "valid_answer_data = [value[1] for value in validset_answer_list]\n",
    "\n",
    "n_steps = 4\n",
    "n_features = 1\n",
    "\n",
    "X_train, Y_train, X_valid, Y_valid = [], [], [], []\n",
    "for seq in train_data:\n",
    "    X_train.append(seq[:len(seq)-1] + [-1 for _ in range(5-len(seq))])\n",
    "    Y_train.append(seq[-1])\n",
    "X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "\n",
    "for seq in valid_data:\n",
    "    X_valid.append(seq[:len(seq)-1] + [-1 for _ in range(5-len(seq))])\n",
    "\n",
    "for seq in valid_answer_data:\n",
    "    Y_valid.append(seq[0])\n",
    "X_valid, Y_valid = np.array(X_valid), np.array(Y_valid)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "X_valid = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], n_features))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. Validation of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('Bi-LSTM-DAE-64-0.2-64-tanh-3-0.1127-93.5052.h5')\n",
    "model.summary()\n",
    "\n",
    "correct = 0\n",
    "total = len(X_valid)\n",
    "first = True\n",
    "score = []\n",
    "for validation, gt in tqdm(zip(X_valid, Y_valid), total=len(X_valid)):\n",
    "    validation = np.reshape(validation, (1, n_steps, n_features))\n",
    "    prediction = model.predict(validation, verbose=0)\n",
    "    result = tf.math.top_k(prediction, k=100)\n",
    "    result = result.indices.numpy()\n",
    "    if first:\n",
    "        print(result.shape)\n",
    "        first = False\n",
    "    if gt in result:\n",
    "        correct += 1\n",
    "        # print(validation, gt, np.argwhere(result == gt)[0], result)\n",
    "        score.append(np.argwhere(result == gt)[0][1]+1)\n",
    "    else:\n",
    "        score.append(101)\n",
    "print(f'Top 100 Acc : {correct/total:.4f}, Avg Rank : {sum(score)/len(score):.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Single Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(n_steps, n_features)))\n",
    "model.add(tf.keras.layers.Masking(mask_value=-1, input_shape=(n_features, )))\n",
    "model.add(Bidirectional(LSTM(64, activation='tanh')))\n",
    "model.add(Dense(42563, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Is Adadelta useful in this task? ...\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "class PredictionCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        correct = 0\n",
    "        total = len(X_valid)\n",
    "        first = True\n",
    "        score = []\n",
    "        for validation, gt in tqdm(zip(X_valid, Y_valid), total=len(X_valid)):\n",
    "            validation = np.reshape(validation, (1, n_steps, n_features))\n",
    "            prediction = model.predict(validation, verbose=0)\n",
    "            result = tf.math.top_k(prediction, k=100)\n",
    "            result = result.indices.numpy()\n",
    "            if first:\n",
    "                print(result.shape)\n",
    "                first = False\n",
    "            if gt in result:\n",
    "                correct += 1\n",
    "                # print(validation, gt, np.argwhere(result == gt)[0], result)\n",
    "                score.append(np.argwhere(result == gt)[0][1]+1)\n",
    "            else:\n",
    "                score.append(101)\n",
    "        print(f'Top 100 Acc : {correct/total:.4f}, Avg Rank : {sum(score)/len(score):.4f}')\n",
    "        model.save(f'./Checkpoints/Bi-LSTM-64-tanh-{epoch}-{correct/total:.4f}-{sum(score)/len(score):.4f}.h5')\n",
    "        gc.collect()\n",
    "        return super().on_epoch_end(epoch, logs)\n",
    "\n",
    "history = model.fit(X_train, Y_train, shuffle=True, epochs=20, callbacks=[PredictionCallback()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. DAE based on the LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Encoder\n",
    "model.add(tf.keras.layers.Masking(mask_value=-1., input_shape=(n_steps, n_features)))\n",
    "model.add(Bidirectional(LSTM(64, activation='tanh', return_sequences=True, recurrent_dropout=0.2), input_shape=(n_steps, n_features)))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(Bidirectional(LSTM(32, activation='tanh', return_sequences=True)))\n",
    "\n",
    "# Decoder\n",
    "# model.add(Bidirectional(LSTM(32, activation='tanh', return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64, activation='tanh')))\n",
    "\n",
    "# Classification\n",
    "model.add(Dense(42563, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Is Adadelta useful in this task? ...\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "class PredictionCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        correct = 0\n",
    "        total = len(X_valid)\n",
    "        first = True\n",
    "        score = []\n",
    "        for validation, gt in tqdm(zip(X_valid, Y_valid), total=len(X_valid)):\n",
    "            validation = np.reshape(validation, (1, n_steps, n_features))\n",
    "            prediction = model.predict(validation, verbose=0)\n",
    "            result = tf.math.top_k(prediction, k=100)\n",
    "            result = result.indices.numpy()\n",
    "            if first:\n",
    "                print(result.shape)\n",
    "                first = False\n",
    "            if gt in result:\n",
    "                correct += 1\n",
    "                # print(validation, gt, np.argwhere(result == gt)[0], result)\n",
    "                score.append(np.argwhere(result == gt)[0][1]+1)\n",
    "            else:\n",
    "                score.append(101)\n",
    "        print(f'Top 100 Acc : {correct/total:.4f}, Avg Rank : {sum(score)/len(score):.4f}')\n",
    "        model.save(f'./Checkpoints/Bi-LSTM-DAE-64-0.2-64-tanh-{epoch}-{correct/total:.4f}-{sum(score)/len(score):.4f}.h5')\n",
    "        gc.collect()\n",
    "        return super().on_epoch_end(epoch, logs)\n",
    "\n",
    "history = model.fit(X_train, Y_train, shuffle=True, epochs=40, callbacks=[PredictionCallback()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TSDAE_cuda_11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
